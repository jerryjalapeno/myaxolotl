# this is the huggingface model that contains *.pt, *.safetensors, or *.bin files
# this can also be a relative path to a model on disk
base_model: jerryjalapeno/Llama-2-1b-0-hf
# you can specify an ignore pattern if the model repo contains more than 1 model type (*.pt, etc)
base_model_ignore_patterns:
# if the base_model repo on hf hub doesn't include configuration .json files,
# you can set that here, or leave this empty to default to base_model
base_model_config: jerryjalapeno/Llama-2-1b-0-hf
# you can specify to choose a specific model revision from huggingface hub
model_revision:
# Optional tokenizer configuration override in case you want to use a different tokenizer
# than the one defined in the base model
tokenizer_config:
# If you want to specify the type of model to load, AutoModelForCausalLM is a good choice too
model_type: LlamaForCausalLM
# Corresponding tokenizer for the model AutoTokenizer is a good choice
tokenizer_type: LlamaTokenizer
# Trust remote code for untrusted source
trust_remote_code:
# use_fast option for tokenizer loading from_pretrained, default to True
tokenizer_use_fast:
# Whether to use the legacy tokenizer setting, defaults to True
tokenizer_legacy:
# resize the model embeddings when new tokens are added to multiples of 32
# this is reported to improve training speed on some models
resize_token_embeddings_to_32x:

# whether you are training a 4-bit GPTQ quantized model
gptq: false
gptq_groupsize:  # group size
gptq_model_v1:  # v1 or v2

# this will attempt to quantize the model down to 8 bits and use adam 8 bit optimizer
load_in_8bit: false
# use bitsandbytes 4 bit
load_in_4bit: false

# Use CUDA bf16
bf16: true # bool or 'full' for `bf16_full_eval`. require >=ampere
# Use CUDA fp16
fp16: false
# Use CUDA tf32
tf32: true # require >=ampere

# a list of one or more datasets to finetune the model with
datasets:
  # hf dataset repo | "json" for local dataset, make sure to fill data_files
  - path: #jerryjalapeno/json2test #CHECK
  # The type of prompt to use for training. [alpaca, sharegpt, gpteacher, oasst, reflection]
    type: completion # format | format:<prompt_style> (chat/instruct) | <prompt_strategies>.load_<load_fn>
    ds_type: # Optional[str] (json|arrow|parquet) defines the datatype when path is a file
    data_files: # path to source data files
    shards: #CHECK # number of shards to split data into
    name: # name of dataset configuration to load

  # custom user prompt
  - path: 
    type:
      # the below are defaults. only set what's needed.
      system_prompt: 
      field_system: 
      field_instruction: 
      field_output: 

      # customizable to be single line or multi-line
      system_format: 
      # 'format' can include {input}
      format: 
      # 'no_input_format' cannot include {input}
      no_input_format:

# axolotl attempts to save the dataset as an arrow after packing the data together so
# subsequent training attempts load faster, relative path
dataset_prepared_path:
# push prepared dataset to hub
push_dataset_to_hub: # repo path
# push checkpoints to hub
hub_model_id: jerryjalapeno/Virtual-Heroin-1b-1-hf # repo path to push finetuned model
# how to push checkpoints to hub
# https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments.hub_strategy
hub_strategy: checkpoint
# whether to use hf `use_auth_token` for loading datasets. Useful for fetching private datasets
# required to be true when used in combination with `push_dataset_to_hub`
hf_use_auth_token: true # boolean
# How much of the dataset to set aside as evaluation. 1 = 100%, 0.50 = 50%, etc. 0 for no eval.
val_set_size: 0.00001
# Num shards for whole dataset
dataset_shard_num: #CHECK
# Index of shard to use for whole dataset
dataset_shard_idx: #CHECK

# the maximum length of an input to train with, this should typically be less than 2048
# as most models have a token/context limit of 2048
sequence_len: 4096
# max sequence length to concatenate training samples together up to
# inspired by StackLLaMA. see https://huggingface.co/blog/stackllama#supervised-fine-tuning
# FutureWarning: This will soon be DEPRECATED
max_packed_sequence_len: 
# use efficient multi-packing with block diagonal attention and per sequence position_ids. Recommend set to 'true'
sample_packing: true
# you can set these packing optimizations AFTER starting a training at least once.
# The trainer will provide recommended values for these values.
sample_packing_eff_est:
total_num_tokens:

# if you want to use 'lora' or 'qlora' or leave blank to train all parameters in original model
adapter: 
# if you already have a lora model trained that you want to load, put that here
# lora hyperparameters
lora_model_dir:
lora_r: 
lora_alpha: 
lora_dropout: 
lora_target_modules:
#  - k_proj
#  - o_proj
#  - gate_proj
#  - down_proj
#  - up_proj
lora_target_linear: # if true, will target all linear layers
lora_modules_to_save:
#  - embed_tokens
#  - lm_head
lora_out_dir:
lora_fan_in_fan_out: false

# ReLoRA configuration
# must use either 'lora' or 'qlora' adapter, and does not support fsdp or deepspeed
relora_steps: # number of steps per ReLoRA restart
relora_warmup_steps: # number of per-restart warmup steps
relora_cpu_offload: # true to perform lora weight merges on cpu during restarts, for modest gpu memory savings

# wandb configuration if you're using it
wandb_mode: # "offline" to save run metadata locally and not sync to the server, "disabled" to turn off wandb
wandb_project: virtual_heroin_1b_base # your wandb project name
wandb_entity: nart # a wandb Team name if using a Team
wandb_watch:
wandb_run_id: test-run-1 # set the name of your wandb run
wandb_log_model: checkpoint # "checkpoint" to log model to wandb Artifacts every `save_steps` or "end" to log only at the end of training

# where to save the finished model to
output_dir: ./completed-model

# training hyperparameters
gradient_accumulation_steps: 1 #2 #CHECK
micro_batch_size: 1 #4 #CHECK
eval_batch_size: 1 #4 #CHECK
num_epochs: #CHECK
warmup_steps: 100 #10000
learning_rate: 0.0006
lr_quadratic_warmup:
logging_steps: 100 #CHECK
save_strategy: # set to `no` to skip checkpoint saves
save_steps: # leave empty to save at each epoch
eval_steps: 1000 #CHECK
save_total_limit: # checkpoints saved at a time
max_steps:

# save model as safetensors (require safetensors package)
save_safetensors:

# whether to mask out or include the human's prompt from the training labels
train_on_inputs: false
# group similarly sized data to minimize padding
# may be slower to start, as it must download and sort the entire dataset
# note that training loss may have an oscillating pattern with this enabled
group_by_length: false

# Whether to use gradient checkpointing https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing
gradient_checkpointing: false

# stop training after this many evaluation losses have increased in a row
# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback
early_stopping_patience: 5

# specify a scheduler and kwargs to use with the optimizer
lr_scheduler: # 'one_cycle' | 'log_sweep' | empty for cosine
lr_scheduler_kwargs:

# for one_cycle optim
lr_div_factor: # learning rate div factor

# for log_sweep optim
log_sweep_min_lr:
log_sweep_max_lr:

# specify optimizer
optimizer: adamw_bnb_8bit
# specify weight decay
weight_decay: 0.1
# adamw hyperparams
adam_beta1: .9
adam_beta2: .95
adam_epsilon: 0.00001
# Gradient clipping max norm
max_grad_norm: 1.0

# whether to bettertransformers
flash_optimum:
# whether to use xformers attention patch https://github.com/facebookresearch/xformers:
xformers_attention:
# whether to use flash attention patch https://github.com/Dao-AILab/flash-attention:
flash_attention: true
# whether to use scaled-dot-product attention
# https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html
sdp_attention:
# Landmark attention (only llama)
landmark_attention:
# xpos RoPE see https://github.com/kaiokendev/cutoff-len-is-context-len/blob/main/util/xpos_rope_llama_monkey_patch.py
# llama only
xpos_rope:
# RoPE Scaling https://github.com/huggingface/transformers/pull/24653
rope_scaling:
  type: # linear | dynamic
  factor: # float

# resume from a specific checkpoint dir
resume_from_checkpoint:
# if resume_from_checkpoint isn't set and you simply want it to start where it left off
# be careful with this being turned on between different models
auto_resume_from_checkpoints: false

# don't mess with this, it's here for accelerate and torchrun
local_rank:

# add or change special tokens
special_tokens:
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
# add extra tokens
tokens:

# FSDP
fsdp:
fsdp_config:

# Deepspeed config path
deepspeed:

# Path to torch distx for optim 'adamw_anyprecision'
torchdistx_path:

# Set padding for data collator to 'longest'
collator_pad_to_longest:

# Set to HF dataset for type: 'completion' for streaming instead of pre-tokenize
pretraining_dataset: jerryjalapeno/jsonl2test

# Debug mode
debug:

# Seed
seed:

# Allow overwrite yml config using from cli
strict: